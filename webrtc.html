<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Realtime WebRTC Debug</title>
  <style>
    body { font-family: system-ui, sans-serif; padding: 24px; }
    #voiceToggle { padding: 10px 16px; font-size: 16px; }
    #log { margin-top: 16px; white-space: pre-wrap; font-family: ui-monospace, SFMono-Regular, monospace; }
  </style>
</head>
<body>
  <button id="voiceToggle">Start</button>
  <audio id="assistantAudio" autoplay playsinline></audio>
  <div id="log"></div>

  <script type="module">
    const logEl = document.getElementById('log');
    const log = (...a)=>{ const s=a.map(x=>typeof x==='string'?x:JSON.stringify(x,null,2)).join(' ');
                          logEl.textContent += s + '\n'; console.log(...a); };

    let pc = null, mic = null;
    const btn = document.getElementById('voiceToggle');
    const audioEl = document.getElementById('assistantAudio');

    async function startRealtime(){
      try {
        // 1) Permission hint
        try {
          if ('permissions' in navigator && navigator.permissions?.query) {
            const p = await navigator.permissions.query({ name: 'microphone' });
            log('[PERM] microphone:', p.state);
          }
        } catch {}

        // 2) Get mic
        log('[RT] getUserMedia…');
        mic = await navigator.mediaDevices.getUserMedia({ audio: true });
        const micTrack = mic.getTracks()[0];
        log('[MIC] track state:', micTrack?.readyState || 'none');

        // 3) PeerConnection + event hooks
        pc = new RTCPeerConnection();
        pc.oniceconnectionstatechange = ()=> log('ICE:', pc.iceConnectionState);
        pc.onconnectionstatechange   = ()=> log('PC:',  pc.connectionState);
        pc.ontrack = (e)=>{
          log('[RT] ontrack: remote stream received');
          audioEl.srcObject = e.streams[0];
          audioEl.play().catch(err=>log('[AUDIO] play() blocked:', err?.message||err));
        };

        // 4) Attach audio ONCE via transceiver, and associate the stream
        const tx = pc.addTransceiver('audio', { direction: 'sendrecv' });
        try { await tx.sender.replaceTrack(micTrack); } catch {}
        if (tx.sender.setStreams) { try { tx.sender.setStreams(mic); } catch {} }

        // (Optional) Data channel -> session settings
        const dc = pc.createDataChannel('oai-events');
        dc.onopen = () => {
  log('[RT] data channel open; sending session update');
  try {
    dc.send(JSON.stringify({
      type: 'session.update',
      session: {
        voice: 'cedar',
        instructions: 'Speak English only, warm tone, concise replies.'
      }
    }));
    dc.send(JSON.stringify({
      type: 'response.create',
      response: { instructions: 'Hello! How are you doing today?' }
    }));
  } catch (e) {
    log('[DC] send error:', e?.message || e);
  }
};


        // 5) Debug current state
        log('Senders:', pc.getSenders().map(s => s.track?.kind || 'none'));
        log('Transceivers:', pc.getTransceivers().map(x => ({ direction: x.direction, mid: x.mid })));

        // 6) Create offer (include legacy hint)
        const offer = await pc.createOffer({ offerToReceiveAudio: 1 });
        await pc.setLocalDescription(offer);

        // ---- SDP LOGGING (this is what we need) ----
        const sdp = offer.sdp || '';
        log('[RT] SDP offer (first 5000 chars):\n' + sdp.slice(0, 5000));

        // Extract just the audio media section for clarity
        const audioSection = (() => {
          const start = sdp.indexOf('\nm=audio ');
          if (start === -1) return '';
          const nextM = sdp.indexOf('\nm=', start + 1);
          return sdp.slice(start + 1, nextM === -1 ? sdp.length : nextM);
        })();
        log('[RT] AUDIO SECTION >>>\n' + (audioSection || '<<none found>>'));

        // Quick checks
        log('[CHK] has m=audio:', sdp.includes('\nm=audio '));
        log('[CHK] has opus codec:', sdp.includes('a=rtpmap:111 opus/48000/2'));
        log('[CHK] has msid:', sdp.includes('a=msid:'));
        log('[CHK] has direction (send/recv):', /a=(sendrecv|sendonly|recvonly)/.test(audioSection));

        // 7) Send offer to your proxy -> OpenAI
        log('[RT] posting offer to https://va-button-test01.vercel.app//api/realtime/offer');
        const res = await fetch('https://va-button-test01.vercel.app//api/realtime/offer', {
          method: 'POST',
          headers: { 'Content-Type': 'application/sdp' },
          body: offer.sdp
        });

        const text = await res.text();
        log('[RT] offer->answer status:', res.status);

        if (!res.ok) {
          try { log('[ERR] OpenAI Realtime error:', JSON.stringify(JSON.parse(text), null, 2)); }
          catch { log('[ERR] OpenAI Realtime error text:', text.slice(0,300)); }
          throw new Error('Offer->Answer failed');
        }

        // 8) Apply answer
        await pc.setRemoteDescription({ type: 'answer', sdp: text });
        log('[RT] remote description set; waiting for audio…');

      } catch (e) {
        log('[EXC] startRealtime error:', e?.message || e);
        throw e;
      }
    }

    function stopRealtime(){
      try { pc?.getSenders().forEach(s => s.track?.stop()); pc?.close(); } catch {}
      pc = null;
      try { mic?.getTracks().forEach(t => t.stop()); } catch {}
      mic = null;
      audioEl.srcObject = null;
      log('[RT] stopped');
    }
