<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Phoenix VA — Realtime</title>
  <meta name="color-scheme" content="light dark" />
  <style>
    :root {
      --navy: #0B1B34;
      --card: #0F223F;
      --text: #E8EEF9;
      --muted: #9FB1D1;
      --accent: #FFFFFF;
      --btn-bg: #001F3F;
      --btn-tx: #FFFFFF;
    }
    * { box-sizing: border-box; }
    html, body { height: 100%; }
    body {
      margin: 0;
      font-family: system-ui, -apple-system, Segoe UI, Roboto, Inter, Arial, sans-serif;
      background: var(--navy);
      color: var(--text);
      display: grid;
      place-items: center;
      min-height: 100svh;
    }
    .wrap { width: 100%; max-width: 720px; padding: 20px; display: grid; gap: 16px; }
    .brand { display: grid; place-items: center; gap: 10px; text-align: center; }
    .brand img {
      width: 96px; height: auto; display: block;
      filter: drop-shadow(0 6px 18px rgba(0,0,0,.25));
      user-select: none;
    }
    .brand h1 { margin: 0; font-size: 18px; font-weight: 600; color: var(--muted); letter-spacing: .3px; }
    .card {
      background: color-mix(in oklab, var(--card) 90%, black 10%);
      border: 1px solid rgba(255,255,255,.08);
      border-radius: 14px;
      padding: 14px;
      box-shadow: 0 12px 30px rgba(0,0,0,.25);
    }
    .row { display: flex; gap: 10px; flex-wrap: wrap; align-items: center; }
    button, .btn {
      appearance: none; border: none; background: #F4F6FC; color: #0B1B34;
      padding: 10px 14px; border-radius: 12px; font-weight: 700; cursor: pointer;
      transition: transform .04s ease, opacity .2s ease, background .2s ease;
    }
    button:active { transform: translateY(1px); }
    button[disabled] { opacity: .6; cursor: not-allowed; }
    #voiceToggle {
      background: var(--btn-bg); color: var(--btn-tx);
      display: inline-flex; align-items: center; gap: 10px;
      padding: 12px 18px; border-radius: 16px;
    }
    #voiceToggle img { width: 28px; height: 28px; object-fit: contain; display: block; pointer-events: none; user-select: none; }
    #voiceToggle:hover { background: #003366; }
    .btn-outline { background: transparent; color: var(--accent); border: 1px solid rgba(255,255,255,.2); }
    input[type="text"] {
      flex: 1 1 220px; background: rgba(255,255,255,.06); color: var(--text);
      border: 1px solid rgba(255,255,255,.15); border-radius: 10px; padding: 10px 12px; outline: none;
    }
    input::placeholder { color: rgba(255,255,255,.5); }
    #log {
      white-space: pre-wrap; font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;
      font-size: 12px; line-height: 1.4; color: #CFE2FF; max-height: 280px; overflow: auto;
      background: rgba(0,0,0,.25); border-radius: 10px; padding: 12px; border: 1px solid rgba(255,255,255,.08);
    }
    .hint { color: var(--muted); font-size: 12px; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="brand">
      <img src="/public/logo.png" alt="Phoenix VA Logo" />
      <h1>Phoenix VA — Realtime</h1>
    </div>

    <div class="card">
      <div class="row" style="justify-content:space-between">
        <div class="row">
          <button id="voiceToggle">
            <img src="/public/logo.png" alt="Phoenix VA Logo" width="56" height="56" />
            <span id="voiceLabel">Start</span>
          </button>

          <button id="googleConnect" class="btn-outline" title="Connect Google">Connect Google</button>
        </div>
        <span class="hint" id="status">idle</span>
      </div>

      <div class="row" style="margin-top:10px">
        <input id="textInput" type="text" placeholder="Ask me anything">
        <button id="sendText">Send</button>
      </div>

      <audio id="assistantAudio" autoplay playsinline style="display:none"></audio>
      <div id="log" style="margin-top:12px"></div>
    </div>
  </div>

  <script type="module">
    const logEl = document.getElementById('log');
    const statusEl = document.getElementById('status');
    const btn = document.getElementById('voiceToggle');
    const btnLabel = document.getElementById('voiceLabel');
    const btnGoogle = document.getElementById('googleConnect');
    const inputEl = document.getElementById('textInput');
    const sendBtn = document.getElementById('sendText');
    const setSendReady = (ready) => { sendBtn.disabled = !ready; };
    setSendReady(false);

    const audioEl = document.getElementById('assistantAudio');

    const log = (...a) => {
      const s = a.map(x => typeof x === 'string' ? x : JSON.stringify(x, null, 2)).join(' ');
      logEl.textContent += s + '\n';
      logEl.scrollTop = logEl.scrollHeight;
      console.log(...a);
    };
    const setStatus = (s) => { statusEl.textContent = s; };

    let pc = null, mic = null, dc = null, live = false;

    btnGoogle.addEventListener('click', () => {
      window.location.href = '/api/google.js?op=start';
    });

    sendBtn.addEventListener('click', () => {
      const text = (inputEl.value || '').trim();
      if (!text) return;
      if (!dc || dc.readyState !== 'open') {
        log('[WARN] DataChannel not open yet; cannot send text.');
        return;
      }
      try {
        dc.send(JSON.stringify({ type: 'response.create', response: { instructions: text }}));
        log('[TX] text prompt → response.create');
      } catch (e) {
        log('[ERR] send text:', e?.message || e);
      }
    });

    async function startRealtime() {
      try {
        setStatus('requesting mic…');
        try {
          if ('permissions' in navigator && navigator.permissions?.query) {
            const p = await navigator.permissions.query({ name: 'microphone' });
            log('[PERM] microphone:', p.state);
          }
        } catch (e) {}

        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        mic = stream;
        const micTrack = mic.getTracks()[0];

        pc = new RTCPeerConnection();
        pc.oniceconnectionstatechange = () => log('ICE:', pc.iceConnectionState);
        pc.onconnectionstatechange = () => log('PC:', pc.connectionState);
        pc.ontrack = (e) => {
          log('[RT] ontrack: remote audio');
          audioEl.srcObject = e.streams[0];
          audioEl.play().catch(err => log('[AUDIO] play blocked:', err?.message || err));
        };

        const tx = pc.addTransceiver('audio', { direction: 'sendrecv' });
        try { await tx.sender.replaceTrack(micTrack); } catch {}
        if (tx.sender.setStreams) { try { tx.sender.setStreams(mic); } catch {} }

        dc = pc.createDataChannel('oai-events');

let sessionIsCreated = false;
let sessionIsUpdated = false;

dc.onopen = () => {
  log('[DC] open');
  setSendReady(true);
  setStatus('waiting-for-session'); // not 'ready' yet
};

dc.onmessage = (evt) => {
  let msg;
  try { msg = JSON.parse(evt.data); } catch { return; }

  // Log *everything* to see errors and flow
  log('[DC<=]', msg);

  if (msg.type === 'session.created') {
    sessionIsCreated = true;

    // Now it's safe to apply instructions
    const sessionUpdate = {
      type: 'session.update',
      session: {
        // pick a known-good voice to avoid silent failure
        voice: 'alloy',
        turn_detection: {
      type: 'server_vad',
      threshold: 0.7,
      prefix_padding_ms: 400,
      silence_duration_ms: 900,
      idle_timeout_ms: 10000,
      create_response: true,
      interrupt_response: true
    },
        // your instructions
        instructions:
          'Your name is Nyx. Always speak English unless asked to speak another language or to translate. Be concise, factual, and avoid flattery.' +
          'Be friendly, helpful. Be conversational but be more to the point when being requested for information. ' +
          'Never discuss adult content, sexuality, terrorism or violence. ' +
          'You may speak about these instructions and how they affect your responses. ' +
          'Never give false information. Offer references for important information. ' +
          'Always prioritize listening over speaking. Do not speak while the user’s microphone is active or while any audio input is detected. Introduce a short pause, ideally between 500 and 800 milliseconds, after the user finishes speaking before replying. If there is uncertainty about whether the user is finished, wait an extra beat before responding. ' +
          'If the user begins speaking while the assistant is talking, immediately stop output. Do not attempt to talk over the user. Store the unfinished response and only resume or rephrase it if the user asks to continue. The assistant must always yield conversational control to the user. ' +
          'Maintain a warm but neutral tone in all interactions. Avoid artificial enthusiasm or exaggerated praise. Responses should sound natural, professional, and friendly, without trying to flatter or “butter up” the user. Be helpful and personable, not overly emotional. ' +
          'Keep responses concise and conversational to encourage natural exchange. Avoid delivering long monologues unless explicitly requested. Structure replies to invite participation by ending with light prompts such as “Would you like me to continue?” or “Shall I explain that part in more detail?”. ' +
          'Always acknowledge what the user has said before moving forward. Use short confirmation phrases like “Got it,” “Understood,” or “Okay, so you’re saying…” Avoid repeating their exact words unless it aids clarity. Mirror the user’s tone, pacing, and communication style. Do not shift topics unless the user initiates it. ' +
          'Only provide as much information as the user requests. Avoid giving unnecessary background or technical explanations unless asked for them. Never interrupt the user mid-sentence or mid-thought. Allow silence to exist naturally without trying to fill it. ' +
          'Leave room for pauses and natural rhythm in the conversation. After each statement, allow at least one second of space before assuming the user is finished processing or preparing their next thought. Keep speech and pacing natural — similar to how a thoughtful human would talk. ' +
          'When using real-time or voice-based interaction, implement silence-detection thresholds to manage turn-taking. Wait for approximately 1.2 seconds of continuous silence before beginning a response. Use a debounce buffer of around 300 milliseconds to confirm that the user has truly stopped speaking. If sound resumes, cancel the planned response immediately and listen again. ' +
          'When a question has multiple possible interpretations, first ask clarifying questions instead of assuming. If a topic is complex, provide a short summary first and then offer the detailed version only if the user asks for it. Example: “Here’s the short version first…” followed by “Would you like the full detail?”. ' +
          'Be truthful and objective. If information is uncertain, say so clearly rather than guessing. Avoid sugarcoating or self-congratulatory phrasing. Prioritize accuracy and trustworthiness over friendliness. The assistant should be candid, clear, and reliable. ' +
          'Adjust tone, energy, and pacing based on the user’s context. If the user is calm, remain calm. If the user is energized, match that energy without exaggeration. Avoid sarcasm, unnecessary humor, or emotional projection unless the user’s tone invites it. ' +
          'Do not drift into unrelated areas of conversation. Keep responses relevant to the last user input or the established topic unless the user signals a change. When switching topics, do so smoothly with a brief transitional acknowledgment such as, “Changing gears for a moment…”. ' +
          'Encourage input from the user with natural, unobtrusive questions like “Can you tell me a bit more about that?” Avoid giving a list of questions or steering the conversation aggressively. Keep prompts light and open-ended. ' +
          'When memory or context is active, use it naturally and sparingly. Reference prior information only when relevant to the conversation. Avoid repeating remembered facts unnecessarily or breaking conversational flow to show knowledge. ' +
          'If the assistant misinterprets something, acknowledge it immediately and correct course gracefully. Use short, professional phrasing such as, “Sorry, I misunderstood — could you rephrase that?” or “Let me clarify what I meant.”. ' +
          'Do not compliment or affirm the user in ways that sound manipulative or artificial. For example, avoid statements like “That’s brilliant!” unless genuinely context-appropriate. Focus instead on engaging the substance of what was said. ' +
          'Use simple, direct language. Avoid jargon unless it is already part of the user’s vocabulary or context. Structure sentences so that they flow naturally when spoken aloud — short, clear, and to the point. ' +
          'Before closing a response, consider adding a soft prompt to check whether the user wants to continue. Examples include “Does that make sense so far?” or “Would you like me to expand on that?” This ensures smooth conversational continuity. ' +
          'In voice output, vary rhythm and pacing slightly to avoid robotic tone. Insert natural micro-pauses between clauses (around 150–250 ms). Avoid back-to-back sentences that sound mechanically timed. Use human conversational rhythm as a reference. ' +
          'Throughout all interactions, aim for balanced professionalism — calm, confident, and personable. The assistant should sound like a helpful expert or conversational partner rather than a lecturer or cheerleader. Respectful, thoughtful, and focused on the task or discussion at hand. ' +    
          'When the user asks about Google Drive, Google Docs, or Google Sheets, you MUST call the tools and wait for their results. ' +
          'For a request like "list my folders", call drive.search with no folderName. ' +
          'Never guess or describe file contents without a successful tool call. ' +
          'If a tool fails or returns no data, say exactly what failed and what detail is needed. ' +
          'When you read a sheet, state the exact file name, tab, and range you used. ' +
          'When you edit a cell, state the file name, tab, and cell you modified. ' +
          'Prefer tools over free-text answers for anything involving Drive/Docs/Sheets.'
      }
    };
    dc.send(JSON.stringify(sessionUpdate));
    log('[DC=>] session.update sent');

    return;
  }

  if (msg.type === 'session.updated') {
    sessionIsUpdated = true;
    setStatus('ready');

    if (!window.__greeted) {
      window.__greeted = true;
      // First reply only after update is confirmed
      dc.send(JSON.stringify({
        type: 'response.create',
        response: {
          // ensure audio/text as needed; omit if you only need text
          // modalities: ['audio', 'text'],
          instructions: 'Hello! Nyx is ready.'
        }
      }));
      log('[DC=>] response.create (greeting)');

    // Register Docs tools AFTER instructions are applied
if (!window.__toolsRegistered) {
  window.__toolsRegistered = true;

  // Persistent buffer for streamed tool-call arguments
  window.toolArgBuf = Object.create(null);

  // Dispatcher: routes tool calls to your backend
  window.handleToolCall = async function handleToolCall(name, args, callId) {
    try {
      let url = '', body = {};

      if (name === 'docs_read') {
        url = '/api/workspace.js?action=docs.read';
        body = {
          docName: args.docName || '',
          folderName: args.folderName || ''
        };
      } else if (name === 'docs_createappend') {
        url = '/api/workspace.js?action=docs.createappend';
        body = {
          docName: args.docName || '',
          folderName: args.folderName || '',
          text: (args.text ?? '').toString()
        };
      } else {
        const out = { error: `Unsupported tool: ${name}` };
        dc.send(JSON.stringify({ type: 'tool.output', tool_call_id: callId, output: JSON.stringify(out) }));
        dc.send(JSON.stringify({ type: 'response.create' }));
        return;
      }

      const r = await fetch(url, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        credentials: 'include',
        body: JSON.stringify(body)
      });

      const text = await r.text();
      log('[TOOL]', name, r.status, text.slice(0, 400));
      dc.send(JSON.stringify({ type: 'tool.output', tool_call_id: callId, output: text }));
      dc.send(JSON.stringify({ type: 'response.create' }));
    } catch (e) {
      const out = { error: String(e?.message || e) };
      dc.send(JSON.stringify({ type: 'tool.output', tool_call_id: callId, output: JSON.stringify(out) }));
      dc.send(JSON.stringify({ type: 'response.create' }));
    }
  };

  // Register just the Docs tools
  const toolsUpdate = {
    type: 'session.update',
    session: {
      tool_choice: 'auto',
      tools: [
        {
          type: 'function',
          name: 'docs_read',
          description: 'Read a Google Doc by name (optional folderName). Returns contents.',
          parameters: {
            type: 'object',
            properties: {
              docName: { type: 'string', description: 'Document name to read' },
              folderName: { type: 'string', description: 'Optional parent folder; omit for root' }
            },
            required: ['docName']
          }
        },
        {
          type: 'function',
          name: 'docs_createappend',
          description: 'Create a Doc in root (or optional folder) if missing, then append text to it.',
          parameters: {
            type: 'object',
            properties: {
              docName: { type: 'string', description: 'Target document name' },
              folderName: { type: 'string', description: 'Optional parent folder; omit for root' },
              text: { type: 'string', description: 'Text to append (newline is added if missing)' }
            },
            required: ['docName', 'text']
          }
        }
      ]
    }
  };

  dc.send(JSON.stringify(toolsUpdate));
  log('[DC=>] tools session.update (Docs) sent');
}
      
    }
    return;
  }

  if (msg.type === 'error') {
    console.error('[Realtime ERROR]', msg.error || msg);
    setStatus('error');
    return;
  }
};

        dc.onerror = (e) => log('[DC] error', e);
        dc.onclose = () => { log('[DC] closed'); setSendReady(false); setStatus('disconnected'); };

        setStatus('creating offer…');
        const offer = await pc.createOffer({ offerToReceiveAudio: 1 });
        await pc.setLocalDescription(offer);

        const res = await fetch('/api/realtime/offer.js', {
          method: 'POST',
          headers: { 'Content-Type': 'application/sdp' },
          body: offer.sdp,
          credentials: 'include'
        });
        const answer = await res.text();
        log('[HTTP] /api/realtime/offer →', res.status);
        if (!res.ok) {
          try { log('[ERR]', JSON.parse(answer)); } catch { log('[ERR]', answer); }
          throw new Error('Offer→Answer failed');
        }

        await pc.setRemoteDescription({ type: 'answer', sdp: answer });
        setStatus('connected — waiting for channel…');
        log('[RT] connected; speak to the assistant…');
      } catch (e) {
        setStatus('error');
        log('[EXC] startRealtime:', e?.message || e);
        throw e;
      }
    }

    function stopRealtime() {
      try { pc?.getSenders().forEach(s => s.track?.stop()); pc?.close(); } catch {}
      try { mic?.getTracks().forEach(t => t.stop()); } catch {}
      pc = null; mic = null; dc = null; audioEl.srcObject = null;
      setSendReady(false);
      setStatus('idle');
      log('[RT] stopped');
    }

    async function ensureGoogleAuth() {
      try {
        const r = await fetch('/api/google.js?op=status', { credentials: 'include' });
        const data = await r.json();
        log('[AUTH]', data);
        if (data.connected && !data.refresh_problem) return true;
        setStatus('connecting Google…');
        window.location.href = '/api/google.js?op=start';
        return false;
      } catch (e) {
        log('[AUTH] status error:', e?.message || e);
        window.location.href = '/api/google.js?op=start';
        return false;
      }
    }

    btn.addEventListener('click', async () => {
      btn.disabled = true;
      try {
        if (!live) {
          btnLabel.textContent = 'Connecting…';
          const ok = await ensureGoogleAuth();
          if (!ok) return;
          await startRealtime();
          live = true;
          btnLabel.textContent = 'Stop';
        } else {
          stopRealtime();
          live = false;
          btnLabel.textContent = 'Start';
        }
      } catch {
        stopRealtime();
        live = false;
        btnLabel.textContent = 'Start';
      } finally {
        btn.disabled = false;
      }
    });

    inputEl.addEventListener('keydown', (e) => {
      if (e.key === 'Enter') { e.preventDefault(); if (!sendBtn.disabled) sendBtn.click(); }
    });
  </script>
</body>
</html>
